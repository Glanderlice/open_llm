{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-24T05:03:58.113009Z",
     "start_time": "2024-08-24T05:03:58.106817Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from utils.toolkit import set_seed, auto_device\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "set_seed(12345)\n",
    "device = auto_device()\n",
    "model_path = 'D:/PycharmProjects/llama3_proj/models/Meta-Llama-3-8B-Instruct'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T04:30:00.274367Z",
     "start_time": "2024-08-24T04:30:00.226805Z"
    }
   },
   "id": "c3488072ef9203af",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # 这个从modelscope下载的llama3与HF不太一样\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def get_processor(tokenizer, shift=False):\n",
    "    def process_function(sample):\n",
    "        prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{sample['instruction']}{sample['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        prompt = tokenizer.encode(prompt, add_special_tokens=False)  # optional: +tokenizer.eos_token\n",
    "        output = tokenizer.encode(f\"{sample['output']}<|eot_id|>\",\n",
    "                                  add_special_tokens=False)  # optional: +tokenizer.eos_token\n",
    "        input_ids = prompt + output\n",
    "        labels = [-100] * len(prompt) + output\n",
    "        if shift:\n",
    "            input_ids = input_ids[:-1]\n",
    "            labels = labels[1:]\n",
    "        sample = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": [1] * len(input_ids),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return sample\n",
    "    return process_function"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T04:34:23.990469Z",
     "start_time": "2024-08-24T04:34:23.809011Z"
    }
   },
   "id": "d4fb17cd55eb7a93",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3729 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db4b8a5af03941b3ada42065b43f7b2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess = get_processor(tokenizer, shift=True)\n",
    "\n",
    "ds = datasets.load_dataset('json', data_files={'train': 'D:/PycharmProjects/open_llm/dataset/huanhuan.json'})\n",
    "train_dataset = ds[\"train\"]\n",
    "train_dataset = ds[\"train\"].map(preprocess, remove_columns=train_dataset.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T04:34:28.304314Z",
     "start_time": "2024-08-24T04:34:26.462191Z"
    }
   },
   "id": "6cb840156e22b604",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128006, 882, 128007, 271, 111319, 3922, 64022, 9554, 106241, 58850, 72368, 19000, 32018, 16325, 31867, 3922, 113723, 19361, 100389, 109, 80578, 111319, 101067, 101307, 58843, 224, 104241, 45829, 3922, 86894, 102, 113715, 111419, 41914, 50928, 89151, 89151, 103203, 9554, 8713, 128009, 128006, 78191, 128007, 271, 103001, 246, 8713, 72368, 37687, 104894, 110767, 37687, 105150, 126957, 108298, 9554, 1811], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103001, 246, 8713, 72368, 37687, 104894, 110767, 37687, 105150, 126957, 108298, 9554, 1811, 128009]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T04:34:31.377051Z",
     "start_time": "2024-08-24T04:34:31.365764Z"
    }
   },
   "id": "f9ff9e5bfb4ca3f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128006, 882, 128007, 271, 103624, 104840, 101402, 103242, 102856, 104587, 21043, 102491, 107297, 3922, 112471, 124375, 101402, 103242, 115820, 105600, 51609, 54253, 18184, 105600, 71869, 116749, 104123, 15225, 101171, 231, 123594, 103429, 102981, 105987, 53901, 3922, 95598, 36827, 103268, 9953, 102924, 104198, 100389, 109, 80578, 102697, 70349, 110774, 1811, 128009, 128006, 78191, 128007, 271, 112022, 11743, 102, 58543, 101402, 43240, 35287, 101602, 76982, 34208, 104840, 101402, 103242, 31634, 15120, 116057, 107634, 3922, 53901, 53901, 71005, 71005, 112022, 1811], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 112022, 11743, 102, 58543, 101402, 43240, 35287, 101602, 76982, 34208, 104840, 101402, 103242, 31634, 15120, 116057, 107634, 3922, 53901, 53901, 71005, 71005, 112022, 1811, 128009]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T05:14:39.937279Z",
     "start_time": "2024-08-24T05:14:39.931128Z"
    }
   },
   "id": "73eee9b947a7b70c",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class LengthBasedBatchSampler(torch.utils.data.BatchSampler):\n",
    "    \"\"\"\n",
    "    构造一个自定义的batch sampler：它将dataset里所有样本按length升序排序, 然后把相邻的样本按batch_size组成一个batch返回\n",
    "    注意：返回的batch(当batch_size>1)依然长度参差不齐, 还需要后续collate_fn完成padding操作\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, batch_size: int, drop_last: bool, shuffle: bool=True) -> None:\n",
    "        if isinstance(next(iter(data_source)), dict):\n",
    "            first_key = next(iter(next(iter(data_source)).keys()))\n",
    "            self.lengths = [len(d[first_key]) for d in data_source]\n",
    "        else:\n",
    "            self.lengths = [len(d) for d in data_source]\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        ids = np.argsort(self.lengths, kind='mergesort')\n",
    "        if self.drop_last:\n",
    "            ids = ids[:len(ids) // self.batch_size * self.batch_size]\n",
    "\n",
    "        batches = [ids[i:i+self.batch_size] for i in range(0, len(ids), self.batch_size)]\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(batches)\n",
    "\n",
    "        for b in batches:\n",
    "            yield b\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.lengths) // self.batch_size\n",
    "        else:\n",
    "            return len(self.lengths) // self.batch_size + (len(self.lengths) % self.batch_size > 0)\n",
    "        \n",
    "batch_sampler = LengthBasedBatchSampler(train_dataset, batch_size=2, drop_last=True, shuffle=True)\n",
    "collate_fn = DataCollatorForSeq2Seq(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T05:14:58.951761Z",
     "start_time": "2024-08-24T05:14:57.163190Z"
    }
   },
   "id": "7261164a964243a2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=batch_sampler,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T05:15:01.846556Z",
     "start_time": "2024-08-24T05:15:01.833168Z"
    }
   },
   "id": "4dd193e9f2cae7de",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,    882, 128007,    271, 113805,   1811, 128009, 128006,\n",
      "          78191, 128007,    271, 113805,  11571, 128009],\n",
      "        [128000, 128006,    882, 128007,    271, 102856,  11571, 128009, 128006,\n",
      "          78191, 128007,    271, 103001,    246,   8713]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100, 113805,  11571, 128009,   -100],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100, 103001,    246,   8713, 128009]])}\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(train_dataloader)\n",
    "print(next(iterator))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T06:04:01.949259Z",
     "start_time": "2024-08-24T06:04:01.939418Z"
    }
   },
   "id": "ff383f041bcce7ec",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# \n",
    "# \n",
    "# def collate_fn(batch: List, eos_token_id):\n",
    "#     max_len = max(len(item['input_ids']) for item in batch)\n",
    "# \n",
    "#     input_ids = []\n",
    "#     attention_mask = []\n",
    "#     labels = []\n",
    "# \n",
    "#     for item in batch:\n",
    "#         input_id = item['input_ids']\n",
    "#         attention_mask_item = item['attention_mask']\n",
    "#         label = item['labels']\n",
    "# \n",
    "#         # 计算填充长度\n",
    "#         pad_len = max_len - len(input_id)\n",
    "# \n",
    "#         input_ids.append(input_id+[eos_token_id] * pad_len)\n",
    "#         attention_mask.append(attention_mask_item+[0] * pad_len)\n",
    "#         labels.append(label+[eos_token_id] * pad_len)\n",
    "# \n",
    "#     # 将 list 转换为 tensor: torch.tensor(input, dtype=torch.long)\n",
    "#     input_ids = torch.LongTensor(input_ids)\n",
    "#     attention_mask = torch.LongTensor(attention_mask)\n",
    "#     labels = torch.LongTensor(labels)\n",
    "# \n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_mask,\n",
    "#         'labels': labels,\n",
    "#     }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8388f139928d281"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_lr(iterate, max_lr=6e-4, warmup_steps=10, max_steps=50):\n",
    "    # 模拟先经过warmup_steps线性上升到max_lr, 再max_steps余弦平滑下降到min_lr的学习率变化过程, 接下来保持min_lr\n",
    "    min_lr = max_lr * 0.1\n",
    "    if iterate < warmup_steps:\n",
    "        return max_lr * (iterate + 1) / warmup_steps\n",
    "    if iterate > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (iterate - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return max_lr + (max_lr - min_lr) * coeff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T05:41:23.398672Z",
     "start_time": "2024-08-24T05:41:23.395540Z"
    }
   },
   "id": "7db41ed45b30e2a6",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "from model.llama import LlamaTransformer\n",
    "\n",
    "\n",
    "def train_lora(model: LlamaTransformer, dataloader, max_lr=6e-4, warmup_steps=140, max_steps=200):\n",
    "    device = auto_device()\n",
    "\n",
    "    # 设置矩阵乘法精度(不设置的话, 默认是'highest'), 在4090提速x1.25, 但需要注意数据吞吐效率是否跟得上\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # 初始化优化器：参数分组, 并对二维矩阵参数设置了weight_decay\n",
    "    optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "    # 训练循环 training loop: 每个step会使用1个batch的数据更新模型\n",
    "    iterator = iter(dataloader)\n",
    "    for step in range(max_steps):\n",
    "        # 训练部分:training process\n",
    "        t_start = time.time()\n",
    "\n",
    "        model.train()  # 切换成训练模式：影响normalization, dropout等机制\n",
    "\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "        loss_accum = 0.0\n",
    "\n",
    "        batch = next(iterator)\n",
    "        x = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        y = batch['labels'].to(device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss_accum = loss.detach().item()\n",
    "        \n",
    "        loss.backward()  # 反向传播：loss backward之前退出autocasting context\n",
    "\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 学习率动态调整：每个step都动态计算当前step的学习率\n",
    "        lr = get_lr(step, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr  # 动态调整优化器中各参数组的学习率\n",
    "\n",
    "        # 梯度下降: 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.synchronize()  # 这里将等待GPU完成当前batch运算, 用于计时器校准\n",
    "\n",
    "        t_end = time.time()\n",
    "\n",
    "        # 打印日志\n",
    "        dt = t_end - t_start  # 单位：秒\n",
    "        print(\n",
    "            f\"step {step}, loss: {loss_accum}, lr: {lr}, norm: {norm}, dt: {dt * 1000:.2f}ms\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T06:03:22.957871Z",
     "start_time": "2024-08-24T06:03:22.951524Z"
    }
   },
   "id": "633ef31a615db44",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained model: meta-llama/Meta-Llama-3-8B => {'n_layers': 32, 'n_heads': 32, 'dim': 4096, 'n_kv_heads': 8, 'vocab_size': 128256, 'max_seq_len': 2048}\n",
      "parameters num: 291\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "debb2fd0765f4af4a375773214471bab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:00<00:00, 382.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "Layer Name & Parameters\n",
      "----------------------------\n",
      "model.embed_tokens.weight                          | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.0.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.0.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.1.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.1.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.2.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.2.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.3.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.3.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.4.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.4.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.5.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.5.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.6.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.6.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.7.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.7.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.8.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.8.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.self_attn.q_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.q_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.k_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.self_attn.k_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.k_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.self_attn.v_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.v_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.o_proj.base_layer.weight  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.self_attn.o_proj.lora_A.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.self_attn.o_proj.lora_B.weight      | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.gate_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.mlp.gate_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.gate_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.up_proj.base_layer.weight       | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.mlp.up_proj.lora_A.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.up_proj.lora_B.weight           | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.down_proj.base_layer.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.mlp.down_proj.lora_A.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.mlp.down_proj.lora_B.weight         | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.9.input_layernorm.weight              | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.9.post_attention_layernorm.weight     | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.10.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.10.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.11.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.11.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.12.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.12.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.13.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.13.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.14.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.14.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.15.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.15.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.16.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.16.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.17.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.17.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.18.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.18.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.19.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.19.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.20.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.20.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.21.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.21.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.22.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.22.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.23.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.23.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.24.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.24.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.25.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.25.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.26.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.26.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.27.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.27.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.28.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.28.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.29.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.29.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.30.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.30.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.self_attn.q_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.q_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.k_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.self_attn.k_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.k_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.self_attn.v_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.v_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.o_proj.base_layer.weight | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.self_attn.o_proj.lora_A.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.self_attn.o_proj.lora_B.weight     | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.gate_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.mlp.gate_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.gate_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.up_proj.base_layer.weight      | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.mlp.up_proj.lora_A.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.up_proj.lora_B.weight          | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.down_proj.base_layer.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.mlp.down_proj.lora_A.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.mlp.down_proj.lora_B.weight        | dtype:torch.bfloat16 | Requires_grad: True\n",
      "model.layers.31.input_layernorm.weight             | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.layers.31.post_attention_layernorm.weight    | dtype:torch.bfloat16 | Requires_grad: False\n",
      "model.norm.weight                                  | dtype:torch.bfloat16 | Requires_grad: False\n",
      "lm_head.weight                                     | dtype:torch.bfloat16 | Requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "from utils.toolkit import print_trainable_parameters, print_model_all_parameters\n",
    "from train.lora import add_lora\n",
    "from model.llama import LlamaTransformer\n",
    "\n",
    "model = LlamaTransformer.from_pretrained('llama-3-8B', local_path=model_path, torch_type=torch.bfloat16).to('cuda')\n",
    "add_lora(model, alpha=32, target=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], dropout_p=0.05)\n",
    "print_trainable_parameters(model)\n",
    "print_model_all_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T06:01:46.536793Z",
     "start_time": "2024-08-24T06:00:41.082616Z"
    }
   },
   "id": "911715fdca50a63",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 448, with 20,971,520 parameters\n",
      "num non-decayed parameter tensors: 0, with 0 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_lora\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[30], line 26\u001B[0m, in \u001B[0;36mtrain_lora\u001B[1;34m(model, dataloader, max_lr, warmup_steps, max_steps)\u001B[0m\n\u001B[0;32m     22\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# 梯度清零\u001B[39;00m\n\u001B[0;32m     24\u001B[0m loss_accum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 26\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m     27\u001B[0m x, y \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16):\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_lora(model, train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-24T06:03:26.108108Z",
     "start_time": "2024-08-24T06:03:26.066850Z"
    }
   },
   "id": "43f8abc3b32ac5be",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "227f5a0ff40e2a58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
