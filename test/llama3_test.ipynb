{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.检查LlamaTransformer.from_pretrained方法是否能正确加载模型权重到自定义的backbone\n",
    "### 它的推理结果理论上跟AutoModelForCausalLM的差不多"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7678c02058e030b2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T05:47:06.389627Z",
     "start_time": "2024-08-27T05:47:04.450104Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from utils.nn_toolkit import set_seed, auto_device"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "set_seed(12345)\n",
    "device = auto_device()\n",
    "\n",
    "model_path = 'D:/PycharmProjects/llama3_proj/models/Meta-Llama-3-8B'\n",
    "\n",
    "# 构造伪输入\n",
    "test_input = torch.randint(0, 10000, (1, 4)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T05:47:07.623312Z",
     "start_time": "2024-08-27T05:47:07.485060Z"
    }
   },
   "id": "888049ad727dabcf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18ad7e52124a4fdf83e29cf840f8cb38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "tensor([[[ 4.3438,  1.6328, -0.3145,  ..., -6.8750, -6.8750, -6.8750],\n",
      "         [ 5.1250,  6.3125,  2.7500,  ..., -9.3750, -9.3750, -9.3750],\n",
      "         [ 3.9062,  7.4688,  4.6562,  ..., -5.7500, -5.7500, -5.7500],\n",
      "         [ 3.7188,  6.3438,  3.3438,  ..., -7.2188, -7.2188, -7.2188]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 原始model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(device)\n",
    "print(model)\n",
    "\n",
    "# forward并获取最后一个隐层的值, 即im_head的前一层\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    res = model(test_input, output_hidden_states=True)\n",
    "\n",
    "print(res.logits)\n",
    "# print(res.hidden_states[-1][0])\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T05:47:19.127014Z",
     "start_time": "2024-08-27T05:47:10.175372Z"
    }
   },
   "id": "988f71cb4a0883bd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained model: meta-llama/Meta-Llama-3-8B => {'n_layers': 32, 'n_heads': 32, 'dim': 4096, 'n_kv_heads': 8, 'vocab_size': 128256, 'max_seq_len': 2048}\n",
      "parameters num: 291\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a708549ff8ad4a1db97e23e53f60cd58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:00<00:00, 375.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.7031,  6.3750,  3.3750,  ..., -7.1562, -7.1562, -7.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "from model.llama import LlamaTransformer\n",
    "model = LlamaTransformer.from_pretrained(\"llama-3-8B\", model_path, torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(test_input)\n",
    "print(logits)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T05:48:34.706284Z",
     "start_time": "2024-08-27T05:47:24.447405Z"
    }
   },
   "id": "4f6601e5cb6f98ae",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.使用hellaswag数据集来测试LlamaTransformer模型结构以及LlamaTransformer.from_pretrained方法的正确性"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed2876a85942332e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from utils.hellaswag import render_example, iterate_examples\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T06:54:11.291190Z",
     "start_time": "2024-08-27T06:54:08.701783Z"
    }
   },
   "id": "bc7a576d0170da7f",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10042"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = list(enumerate(iterate_examples(\"val\")))\n",
    "len(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T06:54:12.425617Z",
     "start_time": "2024-08-27T06:54:12.356920Z"
    }
   },
   "id": "c3b6efdb1b528872",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    # evaluate the autoregressive loss at all positions\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_shift_tokens = shift_tokens.view(-1)\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    # sum and divide by the number of 1s in the mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    # now we have a loss for each of the 4 completions\n",
    "    # the one with the lowest loss should be the most likely\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T06:54:14.915950Z",
     "start_time": "2024-08-27T06:54:14.898518Z"
    }
   },
   "id": "cce887178329081f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained model: meta-llama/Meta-Llama-3-8B => {'n_layers': 32, 'n_heads': 32, 'dim': 4096, 'n_kv_heads': 8, 'vocab_size': 128256, 'max_seq_len': 2048}\n",
      "parameters num: 291\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d88aee3c659d4fd1a9e5f5dc6f33a3b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:00<00:00, 333.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from model.llama import LlamaTransformer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "local_model = \"D:/PycharmProjects/llama3_proj/models/Meta-Llama-3-8B\"\n",
    "# llama3 = AutoModelForCausalLM.from_pretrained(local_model, torch_dtype=torch.bfloat16).to('cuda')\n",
    "llama3 = LlamaTransformer.from_pretrained(\"llama-3-8B\", local_model, torch_type=torch.bfloat16).to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T06:56:56.878051Z",
     "start_time": "2024-08-27T06:54:20.833316Z"
    }
   },
   "id": "cdb741d5e3587336",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 10042/10042 [10:45<00:00, 15.55example/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HellaSwag accuracy: 7035/10042=0.7006\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_total = 0\n",
    "num_correct_norm = 0\n",
    "\n",
    "llama3.eval()\n",
    "\n",
    "# 创建一个进度条\n",
    "for i, example in tqdm(val, desc=\"Processing\", unit=\"example\"):\n",
    "    # only process examples where i % ddp_world_size == ddp_rank\n",
    "    # render the example into tokens and labels\n",
    "    _, tokens, mask, label = render_example(example)\n",
    "    tokens = tokens.to(\"cuda\")\n",
    "    mask = mask.to(\"cuda\")\n",
    "    # get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            # logits = llama3(tokens).logits  # HF-llama3 => 官方Llama3评测 HellaSwag accuracy: 7037/10042=0.7008\n",
    "            logits, _ = llama3(tokens, fast_inference=False)  # 自定义llama3评测 HellaSwag accuracy: ?\n",
    "        pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "    num_total += 1\n",
    "    num_correct_norm += int(pred_norm == label)\n",
    "acc_norm = num_correct_norm / num_total\n",
    "print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
    "\n",
    "del llama3\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-27T07:07:55.888402Z",
     "start_time": "2024-08-27T06:57:09.851278Z"
    }
   },
   "id": "76ac609e51c35d4f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7cad6a6f6791dff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
